{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XZFrKWHwEf7h"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DsUYd7GhEf7m"
   },
   "source": [
    "Importing the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RC5YKYGuEf7n"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as n\n",
    "import torch.nn.functional as f\n",
    "import numpy as np\n",
    "import os\n",
    "from torchsummary import summary\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from torchvision import models\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2VN1wknpEf7q"
   },
   "source": [
    "Defining the celebrity dataset folder and listing all images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4HEAwzAFEf7r"
   },
   "outputs": [],
   "source": [
    "celebData = \"celeba-dataset/img_align_celeba/img_align_celeba/\"\n",
    "images = os.listdir(celebData)\n",
    "imageList = images[:1500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "dOAZSX60Ef7t",
    "outputId": "75856944-bc8f-4eea-d22c-1d3744657c11"
   },
   "outputs": [],
   "source": [
    "len(imageList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5pvl-X2BEf7w"
   },
   "source": [
    "Assigning the device as cuda if GPU is available else cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3l4EmLQgEf7w"
   },
   "outputs": [],
   "source": [
    "cuda = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wtQvgarfEf7z"
   },
   "source": [
    "Defining the generator architecture as given the reference paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yWoZ2TtvEf7z"
   },
   "outputs": [],
   "source": [
    "class Generator(n.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = n.Conv2d(3,64,9,padding=4,bias=False)\n",
    "        self.conv2 = n.Conv2d(64,64,3,padding=1,bias=False)\n",
    "        self.conv3_1 = n.Conv2d(64,256,3,padding=1,bias=False)\n",
    "        self.conv3_2 = n.Conv2d(64,256,3,padding=1,bias=False)\n",
    "        self.conv4 = n.Conv2d(64,3,9,padding=4,bias=False)\n",
    "        self.bn = n.BatchNorm2d(64)\n",
    "        self.ps = n.PixelShuffle(2)\n",
    "        self.prelu = n.PReLU()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        block1 = self.prelu(self.conv1(x))\n",
    "        block2 = torch.add(self.bn(self.conv2(self.prelu(self.bn(self.conv2(block1))))),block1)\n",
    "        block3 = torch.add(self.bn(self.conv2(self.prelu(self.bn(self.conv2(block2))))),block2)\n",
    "        block4 = torch.add(self.bn(self.conv2(self.prelu(self.bn(self.conv2(block3))))),block3)\n",
    "        block5 = torch.add(self.bn(self.conv2(self.prelu(self.bn(self.conv2(block4))))),block4)\n",
    "        block6 = torch.add(self.bn(self.conv2(self.prelu(self.bn(self.conv2(block5))))),block5)\n",
    "        block7 = torch.add(self.bn(self.conv2(block6)),block1)\n",
    "        block8 = self.prelu(self.ps(self.conv3_1(block7)))\n",
    "        block9 = self.prelu(self.ps(self.conv3_2(block8)))\n",
    "        block10 = self.conv4(block9)\n",
    "        return block10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yVcX-zDaEf71"
   },
   "source": [
    "Assigning the Generator to cuda(if available) and printing the summary of the Generator with a dummy input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kBHl0rO_Ef72"
   },
   "outputs": [],
   "source": [
    "gen = Generator().to(cuda)\n",
    "\n",
    "#Uncomment below mentioned three lines if you have more than one gpu and want to use all of them\n",
    "#ngpu=2\n",
    "# if (cuda.type == 'cuda') and (ngpu > 1):\n",
    "#     gen = n.DataParallel(gen, list(range(ngpu)))\n",
    "summary(gen,(3,64,64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nbQiATSjEf74"
   },
   "source": [
    "Defining the Discriminator network as given in the reference paper expect that a dropout in the last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DAQ63HYsEf75"
   },
   "outputs": [],
   "source": [
    "class Discriminator(n.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = n.Conv2d(3,64,3,padding=1,bias=False)\n",
    "        self.conv2 = n.Conv2d(64,64,3,stride=2,padding=1,bias=False)\n",
    "        self.bn2 = n.BatchNorm2d(64)\n",
    "        self.conv3 = n.Conv2d(64,128,3,padding=1,bias=False)\n",
    "        self.bn3 = n.BatchNorm2d(128)\n",
    "        self.conv4 = n.Conv2d(128,128,3,stride=2,padding=1,bias=False)\n",
    "        self.bn4 = n.BatchNorm2d(128)\n",
    "        self.conv5 = n.Conv2d(128,256,3,padding=1,bias=False)\n",
    "        self.bn5 = n.BatchNorm2d(256)\n",
    "        self.conv6 = n.Conv2d(256,256,3,stride=2,padding=1,bias=False)\n",
    "        self.bn6 = n.BatchNorm2d(256)\n",
    "        self.conv7 = n.Conv2d(256,512,3,padding=1,bias=False)\n",
    "        self.bn7 = n.BatchNorm2d(512)\n",
    "        self.conv8 = n.Conv2d(512,512,3,stride=2,padding=1,bias=False)\n",
    "        self.bn8 = n.BatchNorm2d(512)\n",
    "        self.fc1 = n.Linear(512*16*16,1024)\n",
    "        self.fc2 = n.Linear(1024,1)\n",
    "        self.drop = n.Dropout2d(0.3)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        block1 = f.leaky_relu(self.conv1(x))\n",
    "        block2 = f.leaky_relu(self.bn2(self.conv2(block1)))\n",
    "        block3 = f.leaky_relu(self.bn3(self.conv3(block2)))\n",
    "        block4 = f.leaky_relu(self.bn4(self.conv4(block3)))\n",
    "        block5 = f.leaky_relu(self.bn5(self.conv5(block4)))\n",
    "        block6 = f.leaky_relu(self.bn6(self.conv6(block5)))\n",
    "        block7 = f.leaky_relu(self.bn7(self.conv7(block6)))\n",
    "        block8 = f.leaky_relu(self.bn8(self.conv8(block7)))\n",
    "        block8 = block8.view(-1,block8.size(1)*block8.size(2)*block8.size(3))\n",
    "        block9 = f.leaky_relu(self.fc1(block8),)\n",
    "#         block9 = block9.view(-1,block9.size(1)*block9.size(2)*block9.size(3))\n",
    "        block10 = torch.sigmoid(self.drop(self.fc2(block9)))\n",
    "        return block9,block10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k2ub9ArZEf76"
   },
   "source": [
    "Assigning the discriminator to the gpu(if available) and printing the summary of the network with a dummy value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9GXi9cbgEf77"
   },
   "outputs": [],
   "source": [
    "disc = Discriminator().to(cuda)\n",
    "summary(disc,(3,256,256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_2JfIT4bEf79"
   },
   "outputs": [],
   "source": [
    "disc = Discriminator().to(cuda).float()\n",
    "gen = Generator().to(cuda).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kVO4XWIFEf7_"
   },
   "source": [
    "Downloading the pretrained vgg19 model from model module of torchvision library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KCbJX4oyEf7_"
   },
   "outputs": [],
   "source": [
    "vgg = models.vgg19(pretrained=True).to(cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MLIIT_rvEf8B"
   },
   "source": [
    "Defining the losses to be used in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rArq8bpwEf8B"
   },
   "outputs": [],
   "source": [
    "gen_loss = n.BCELoss()\n",
    "vgg_loss = n.MSELoss()\n",
    "mse_loss = n.MSELoss()\n",
    "disc_loss = n.BCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KpqcTlWKEf8D"
   },
   "source": [
    "Defining the adam optimizers for generator and discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dNyZ946bEf8D"
   },
   "outputs": [],
   "source": [
    "gen_optimizer = optim.Adam(gen.parameters(),lr=0.0001)\n",
    "disc_optimizer = optim.Adam(disc.parameters(),lr=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pN7aKN62Ef8G"
   },
   "source": [
    "Loading the images after based on resizing as numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GR_RwD4zEf8G"
   },
   "outputs": [],
   "source": [
    "def loadImages(imageList,path,resize=False):\n",
    "    images=[]\n",
    "    for image in (imageList):\n",
    "#         print(image)\n",
    "        if resize:\n",
    "            img = cv2.resize(cv2.imread(os.path.join(path,image)),(256,256)) \n",
    "        else:\n",
    "            img = cv2.imread(os.path.join(path,image))\n",
    "#         img = img.reshape(img.shape[2],img.shape[0],img.shape[1])\n",
    "#         print(img.shape)\n",
    "        img = np.moveaxis(img, 2, 0)\n",
    "#         print(img.shape)\n",
    "        images.append(img)\n",
    "    return np.array(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8PnAJcu1Ef8I"
   },
   "source": [
    "Converting the high resolution images into low resolution by applying gaussian blur, resizing in to 64*64 and loading it as numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-nv01VaFEf8I"
   },
   "outputs": [],
   "source": [
    "def loadLRImages(imagelist,path):\n",
    "    images=[]\n",
    "    for image in (imagelist):\n",
    "        img = cv2.resize(cv2.GaussianBlur(cv2.imread(os.path.join(path,image)),(5,5),cv2.BORDER_DEFAULT),(64,64)) \n",
    "#         img = img.reshape(img.shape[2],img.shape[0],img.shape[1])\n",
    "        img = np.moveaxis(img, 2, 0)\n",
    "        images.append(img)\n",
    "    return np.array(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xhpVOaMvEf8L"
   },
   "source": [
    "Loading the generator model from the given checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uoI7nAl8Ef8L"
   },
   "outputs": [],
   "source": [
    "def load_checkpoint(filepath):\n",
    "    checkpoint = torch.load(filepath)\n",
    "    model = checkpoint['model']\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    for parameter in model.parameters():\n",
    "        parameter.requires_grad = False\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fGQDWlS0Ef8N"
   },
   "source": [
    "Given low resolution images and checkpoint, Generating the high resolution images out of it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_MU7hpP6Ef8N"
   },
   "outputs": [],
   "source": [
    "def imagePostProcess(imagedir,modelPath):\n",
    "    imagelist=[]\n",
    "#     images = os.listdir(imagedir)\n",
    "    for img in imagedir:\n",
    "        img = cv2.resize(cv2.GaussianBlur(cv2.imread(os.path.join(hr_path,img)),(5,5),cv2.BORDER_DEFAULT),(64,64)) \n",
    "        imagelist.append(img)\n",
    "    imagearray = np.array(imagelist)/255\n",
    "#     imagearray = (imagedir)/255\n",
    "    # imagearrayPT = np.reshape(imagearray,(len(imagelist),imagearray.shape[3],imagearray.shape[1],imagearray.shape[2]))\n",
    "    imagearrayPT = np.moveaxis(imagearray,3,1)\n",
    "    # print(imagearrayPT.shape)\n",
    "\n",
    "    model = load_checkpoint(modelPath)\n",
    "    im_tensor = torch.from_numpy(imagearrayPT).float()\n",
    "    out_tensor = model(im_tensor)\n",
    "    # print(out_tensor.shape)\n",
    "    # out = np.reshape(out_tensor,[out_tensor.shape[0],out_tensor.shape[2],out_tensor.shape[3],out_tensor.shape[1]])\n",
    "    out = out_tensor.numpy()\n",
    "    out = np.moveaxis(out,1,3)\n",
    "    # print(out.shape)\n",
    "    out = np.clip(out,0,1)\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LJ_ukw9GEf8P"
   },
   "source": [
    "Display utility of displaying images using matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aB-SFmTlEf8P"
   },
   "outputs": [],
   "source": [
    "def show_samples(sample_images):\n",
    "    figure, axes = plt.subplots(1, sample_images.shape[0], figsize = (10,10))\n",
    "    for index, axis in enumerate(axes):\n",
    "        axis.axis('off')\n",
    "        image_array = sample_images[index]\n",
    "        axis.imshow(image_array)\n",
    "        image = Image.fromarray((image_array * 255).astype('uint8'))\n",
    "    plt.savefig(os.path.join(base_path,\"out/SR\")+\"_\"+str(epoch)+\".png\", bbox_inches='tight', pad_inches=0)\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xZBTT9lBEf8R"
   },
   "outputs": [],
   "source": [
    "#change the batch-size based on your system memory\n",
    "\n",
    "epochs=1000\n",
    "batch_size=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "fzY7B-GCEf8T",
    "outputId": "50e0f9ae-7fce-4333-ac87-7fa8072c68cc"
   },
   "outputs": [],
   "source": [
    "import os \n",
    "base_path = os.getcwd()\n",
    "\n",
    "#lr_path = os.path.join(base_path,\"trainImages\")\n",
    "hr_path =celebData\n",
    "#valid_path = os.path.join(base_path,\"SR_valid\")\n",
    "weight_file = os.path.join(base_path,\"SRPT_weights\")\n",
    "out_path = os.path.join(base_path,\"out\")\n",
    "\n",
    "if not os.path.exists(weight_file):\n",
    "    os.makedirs(weight_file)\n",
    "\n",
    "if not os.path.exists(out_path):\n",
    "    os.makedirs(out_path)\n",
    "\n",
    "    \n",
    "#LR_images_list = os.listdir(lr_path)\n",
    "HR_images_list = imageList\n",
    "batch_count = len(HR_images_list)//batch_size\n",
    "batch_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1pwc5pMhEf8U"
   },
   "source": [
    "Starting of training and defining losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 450
    },
    "colab_type": "code",
    "id": "L1p5wFAzEf8V",
    "outputId": "68224ed4-b26a-4d86-abad-881f7ab1e25f"
   },
   "outputs": [],
   "source": [
    "#batch_count=60\n",
    "for epoch in range(epochs):\n",
    "    d1loss_list=[]\n",
    "    d2loss_list=[]\n",
    "    gloss_list=[]\n",
    "    vloss_list=[]\n",
    "    mloss_list=[]\n",
    "    \n",
    "    for batch in tqdm(range(batch_count)):\n",
    "        hr_imagesList = [img for img in HR_images_list[batch*batch_size:(batch+1)*batch_size]]\n",
    "        lr_images = loadLRImages(hr_imagesList,hr_path)/255\n",
    "        hr_images = loadImages(hr_imagesList,hr_path,True)/255\n",
    "        \n",
    "                \n",
    "        disc.zero_grad()\n",
    "\n",
    "        gen_out = gen(torch.from_numpy(lr_images).to(cuda).float())\n",
    "        _,f_label = disc(gen_out)\n",
    "        _,r_label = disc(torch.from_numpy(hr_images).to(cuda).float())\n",
    "        d1_loss = (disc_loss(f_label,torch.zeros_like(f_label,dtype=torch.float)))\n",
    "        d2_loss = (disc_loss(r_label,torch.ones_like(r_label,dtype=torch.float)))\n",
    "        # d_loss = d1_loss+d2_loss\n",
    "        d2_loss.backward()\n",
    "        d1_loss.backward(retain_graph=True)\n",
    "        # print(d1_loss,d2_loss)\n",
    "#         d_loss.backward(retain_graph=True)\n",
    "        disc_optimizer.step()\n",
    "        \n",
    "\n",
    "        gen.zero_grad()      \n",
    "        g_loss = gen_loss(f_label.data,torch.ones_like(f_label,dtype=torch.float))\n",
    "        v_loss = vgg_loss(vgg.features[:7](gen_out),vgg.features[:7](torch.from_numpy(hr_images).to(cuda).float()))\n",
    "        m_loss = mse_loss(gen_out,torch.from_numpy(hr_images).to(cuda).float())\n",
    "        \n",
    "        generator_loss = g_loss + v_loss + m_loss\n",
    "        # v_loss.backward(retain_graph=True)\n",
    "        # m_loss.backward(retain_graph=True)\n",
    "        # g_loss.backward()\n",
    "        # print(generator_loss)\n",
    "\n",
    "        generator_loss.backward()\n",
    "        gen_optimizer.step()\n",
    "        \n",
    "        d1loss_list.append(d1_loss.item())\n",
    "        d2loss_list.append(d2_loss.item())\n",
    "        \n",
    "        gloss_list.append(g_loss.item())\n",
    "        vloss_list.append(v_loss.item())\n",
    "        mloss_list.append(m_loss.item())\n",
    "\n",
    "        \n",
    "        \n",
    "#         print(\"d1Loss ::: \"+str((d1_loss.item()))+\" d2Loss ::: \"+str((d2_loss.item())))\n",
    "#         print(\"gloss ::: \"+str((g_loss.item()))+\" vloss ::: \"+str((v_loss.item()))+\" mloss ::: \"+str((m_loss.item())))\n",
    "    print(\"Epoch ::::  \"+str(epoch+1)+\"  d1_loss ::: \"+str(np.mean(d1loss_list))+\"  d2_loss :::\"+str(np.mean(d2loss_list)))\n",
    "    print(\"genLoss ::: \"+str(np.mean(gloss_list))+\"  vggLoss ::: \"+str(np.mean(vloss_list))+\"  MeanLoss  ::: \"+str(np.mean(mloss_list)))\n",
    "    \n",
    "    if(epoch%3==0):\n",
    "        \n",
    "        checkpoint = {'model': Generator(),\n",
    "              'input_size': 64,\n",
    "              'output_size': 256,\n",
    "              'state_dict': gen.state_dict()}\n",
    "        torch.save(checkpoint,os.path.join(weight_file,\"SR\"+str(epoch+1)+\".pth\"))\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        out_images = imagePostProcess(images[-2:],os.path.join(weight_file,\"SR\"+str(epoch+1)+\".pth\"))\n",
    "#         print(out_images.shape)\n",
    "#         test_images = loadLRImages(images[:-3],hr_path)/255\n",
    "#         test_images = np.reshape(test_images,(test_images[0],test_images.shape[3],test_images.shape[1],test_images.shape[2]))\n",
    "#         out_images = gen(torch.from_numpy(test_images).to(cuda).float())\n",
    "#         out_images = np.reshape(out_images,(out_images[0],out_images[2],out_images[3],out_images[1]))\n",
    "        show_samples(out_images)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "SR_PT.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
